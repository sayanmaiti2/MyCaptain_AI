{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file\n",
    "file = open('frankenstein.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Standardization\n",
    "def tokenize_words(input):\n",
    "    input = input.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \"\".join(filtered)\n",
    "\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Characters to Numbers\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num =dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  241873\n",
      "Total vocab:  42\n"
     ]
    }
   ],
   "source": [
    "# Checking if words to chars to num (?!) has worked\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters: \", input_len)\n",
    "print(\"Total vocab: \", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Length\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  241773\n"
     ]
    }
   ],
   "source": [
    "# Looping through the sequence\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    in_seq = processed_inputs[i: i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input sequence to np_array\n",
    "X = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss ='categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.9444\n",
      "Epoch 00001: loss improved from inf to 2.94441, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 120s 253ms/step - loss: 2.9444\n",
      "Epoch 2/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.9178\n",
      "Epoch 00002: loss improved from 2.94441 to 2.91782, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.9178\n",
      "Epoch 3/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.9145\n",
      "Epoch 00003: loss improved from 2.91782 to 2.91446, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 257ms/step - loss: 2.9145\n",
      "Epoch 4/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.9011\n",
      "Epoch 00004: loss improved from 2.91446 to 2.90106, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.9011\n",
      "Epoch 5/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.8811\n",
      "Epoch 00005: loss improved from 2.90106 to 2.88113, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.8811\n",
      "Epoch 6/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.8615\n",
      "Epoch 00006: loss improved from 2.88113 to 2.86154, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.8615\n",
      "Epoch 7/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.8383\n",
      "Epoch 00007: loss improved from 2.86154 to 2.83829, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.8383\n",
      "Epoch 8/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.8127\n",
      "Epoch 00008: loss improved from 2.83829 to 2.81270, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.8127\n",
      "Epoch 9/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.7847\n",
      "Epoch 00009: loss improved from 2.81270 to 2.78474, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.7847\n",
      "Epoch 10/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.7536\n",
      "Epoch 00010: loss improved from 2.78474 to 2.75359, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.7536\n",
      "Epoch 11/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.7227\n",
      "Epoch 00011: loss improved from 2.75359 to 2.72270, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.7227\n",
      "Epoch 12/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.6923\n",
      "Epoch 00012: loss improved from 2.72270 to 2.69235, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.6923\n",
      "Epoch 13/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.6594\n",
      "Epoch 00013: loss improved from 2.69235 to 2.65940, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.6594\n",
      "Epoch 14/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.6273\n",
      "Epoch 00014: loss improved from 2.65940 to 2.62726, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.6273\n",
      "Epoch 15/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.5983\n",
      "Epoch 00015: loss improved from 2.62726 to 2.59826, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.5983\n",
      "Epoch 16/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.5698\n",
      "Epoch 00016: loss improved from 2.59826 to 2.56981, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 257ms/step - loss: 2.5698\n",
      "Epoch 17/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.5441\n",
      "Epoch 00017: loss improved from 2.56981 to 2.54409, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.5441\n",
      "Epoch 18/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.5211\n",
      "Epoch 00018: loss improved from 2.54409 to 2.52109, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.5211\n",
      "Epoch 19/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.4968\n",
      "Epoch 00019: loss improved from 2.52109 to 2.49678, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.4968\n",
      "Epoch 20/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.4763\n",
      "Epoch 00020: loss improved from 2.49678 to 2.47634, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.4763\n",
      "Epoch 21/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.4579\n",
      "Epoch 00021: loss improved from 2.47634 to 2.45787, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.4579\n",
      "Epoch 22/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.4357\n",
      "Epoch 00022: loss improved from 2.45787 to 2.43571, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.4357\n",
      "Epoch 23/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.4194\n",
      "Epoch 00023: loss improved from 2.43571 to 2.41945, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.4194\n",
      "Epoch 24/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.4022\n",
      "Epoch 00024: loss improved from 2.41945 to 2.40225, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.4022\n",
      "Epoch 25/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.3880\n",
      "Epoch 00025: loss improved from 2.40225 to 2.38803, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.3880\n",
      "Epoch 26/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.3690\n",
      "Epoch 00026: loss improved from 2.38803 to 2.36905, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.3690\n",
      "Epoch 27/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.3544\n",
      "Epoch 00027: loss improved from 2.36905 to 2.35445, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 257ms/step - loss: 2.3544\n",
      "Epoch 28/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.3417\n",
      "Epoch 00028: loss improved from 2.35445 to 2.34166, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 257ms/step - loss: 2.3417\n",
      "Epoch 29/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.3312\n",
      "Epoch 00029: loss improved from 2.34166 to 2.33121, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.3312\n",
      "Epoch 30/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.3135\n",
      "Epoch 00030: loss improved from 2.33121 to 2.31354, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.3135\n",
      "Epoch 31/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2998\n",
      "Epoch 00031: loss improved from 2.31354 to 2.29976, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 257ms/step - loss: 2.2998\n",
      "Epoch 32/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2896\n",
      "Epoch 00032: loss improved from 2.29976 to 2.28960, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.2896\n",
      "Epoch 33/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2778\n",
      "Epoch 00033: loss improved from 2.28960 to 2.27780, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.2778\n",
      "Epoch 34/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2660\n",
      "Epoch 00034: loss improved from 2.27780 to 2.26596, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/473 [==============================] - 121s 255ms/step - loss: 2.2660\n",
      "Epoch 35/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2562\n",
      "Epoch 00035: loss improved from 2.26596 to 2.25622, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.2562\n",
      "Epoch 36/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2457\n",
      "Epoch 00036: loss improved from 2.25622 to 2.24570, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.2457\n",
      "Epoch 37/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2358\n",
      "Epoch 00037: loss improved from 2.24570 to 2.23583, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.2358\n",
      "Epoch 38/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2255\n",
      "Epoch 00038: loss improved from 2.23583 to 2.22552, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.2255\n",
      "Epoch 39/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2161\n",
      "Epoch 00039: loss improved from 2.22552 to 2.21605, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.2161\n",
      "Epoch 40/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.2072\n",
      "Epoch 00040: loss improved from 2.21605 to 2.20723, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.2072\n",
      "Epoch 41/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1985\n",
      "Epoch 00041: loss improved from 2.20723 to 2.19854, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.1985\n",
      "Epoch 42/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1897\n",
      "Epoch 00042: loss improved from 2.19854 to 2.18972, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 257ms/step - loss: 2.1897\n",
      "Epoch 43/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1802\n",
      "Epoch 00043: loss improved from 2.18972 to 2.18016, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.1802\n",
      "Epoch 44/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1715\n",
      "Epoch 00044: loss improved from 2.18016 to 2.17155, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 258ms/step - loss: 2.1715\n",
      "Epoch 45/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1645\n",
      "Epoch 00045: loss improved from 2.17155 to 2.16452, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 257ms/step - loss: 2.1645\n",
      "Epoch 46/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1584\n",
      "Epoch 00046: loss improved from 2.16452 to 2.15844, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.1584\n",
      "Epoch 47/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1493\n",
      "Epoch 00047: loss improved from 2.15844 to 2.14932, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.1493\n",
      "Epoch 48/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1435\n",
      "Epoch 00048: loss improved from 2.14932 to 2.14355, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.1435\n",
      "Epoch 49/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1364\n",
      "Epoch 00049: loss improved from 2.14355 to 2.13638, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.1364\n",
      "Epoch 50/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1277\n",
      "Epoch 00050: loss improved from 2.13638 to 2.12773, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.1277\n",
      "Epoch 51/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1233\n",
      "Epoch 00051: loss improved from 2.12773 to 2.12328, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.1233\n",
      "Epoch 52/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1173\n",
      "Epoch 00052: loss improved from 2.12328 to 2.11731, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.1173\n",
      "Epoch 53/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1105\n",
      "Epoch 00053: loss improved from 2.11731 to 2.11053, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.1105\n",
      "Epoch 54/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.1043\n",
      "Epoch 00054: loss improved from 2.11053 to 2.10433, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.1043\n",
      "Epoch 55/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0991\n",
      "Epoch 00055: loss improved from 2.10433 to 2.09906, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0991\n",
      "Epoch 56/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0943\n",
      "Epoch 00056: loss improved from 2.09906 to 2.09432, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0943\n",
      "Epoch 57/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0857\n",
      "Epoch 00057: loss improved from 2.09432 to 2.08571, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0857\n",
      "Epoch 58/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0843\n",
      "Epoch 00058: loss improved from 2.08571 to 2.08427, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0843\n",
      "Epoch 59/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0757\n",
      "Epoch 00059: loss improved from 2.08427 to 2.07572, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0757\n",
      "Epoch 60/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0723\n",
      "Epoch 00060: loss improved from 2.07572 to 2.07234, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0723\n",
      "Epoch 61/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0681\n",
      "Epoch 00061: loss improved from 2.07234 to 2.06809, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0681\n",
      "Epoch 62/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0638\n",
      "Epoch 00062: loss improved from 2.06809 to 2.06380, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0638\n",
      "Epoch 63/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0598\n",
      "Epoch 00063: loss improved from 2.06380 to 2.05978, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 257ms/step - loss: 2.0598\n",
      "Epoch 64/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0549\n",
      "Epoch 00064: loss improved from 2.05978 to 2.05491, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 257ms/step - loss: 2.0549\n",
      "Epoch 65/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0495\n",
      "Epoch 00065: loss improved from 2.05491 to 2.04950, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 122s 257ms/step - loss: 2.0495\n",
      "Epoch 66/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0427\n",
      "Epoch 00066: loss improved from 2.04950 to 2.04274, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0427\n",
      "Epoch 67/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0408\n",
      "Epoch 00067: loss improved from 2.04274 to 2.04076, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 120s 254ms/step - loss: 2.0408\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/473 [==============================] - ETA: 0s - loss: 2.0342\n",
      "Epoch 00068: loss improved from 2.04076 to 2.03422, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0342\n",
      "Epoch 69/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0317\n",
      "Epoch 00069: loss improved from 2.03422 to 2.03173, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0317\n",
      "Epoch 70/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0283\n",
      "Epoch 00070: loss improved from 2.03173 to 2.02826, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 120s 255ms/step - loss: 2.0283\n",
      "Epoch 71/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0232\n",
      "Epoch 00071: loss improved from 2.02826 to 2.02317, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 120s 254ms/step - loss: 2.0232\n",
      "Epoch 72/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0184\n",
      "Epoch 00072: loss improved from 2.02317 to 2.01840, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0184\n",
      "Epoch 73/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0132\n",
      "Epoch 00073: loss improved from 2.01840 to 2.01320, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0132\n",
      "Epoch 74/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0101\n",
      "Epoch 00074: loss improved from 2.01320 to 2.01014, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 2.0101\n",
      "Epoch 75/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0075\n",
      "Epoch 00075: loss improved from 2.01014 to 2.00749, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0075\n",
      "Epoch 76/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0037\n",
      "Epoch 00076: loss improved from 2.00749 to 2.00373, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0037\n",
      "Epoch 77/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 2.0034\n",
      "Epoch 00077: loss improved from 2.00373 to 2.00345, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 2.0034\n",
      "Epoch 78/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9964\n",
      "Epoch 00078: loss improved from 2.00345 to 1.99638, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 120s 254ms/step - loss: 1.9964\n",
      "Epoch 79/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9921\n",
      "Epoch 00079: loss improved from 1.99638 to 1.99214, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 1.9921\n",
      "Epoch 80/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9906\n",
      "Epoch 00080: loss improved from 1.99214 to 1.99057, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 1.9906\n",
      "Epoch 81/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9871\n",
      "Epoch 00081: loss improved from 1.99057 to 1.98712, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 1.9871\n",
      "Epoch 82/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9874\n",
      "Epoch 00082: loss did not improve from 1.98712\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 1.9874\n",
      "Epoch 83/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9793\n",
      "Epoch 00083: loss improved from 1.98712 to 1.97932, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 1.9793\n",
      "Epoch 84/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9781\n",
      "Epoch 00084: loss improved from 1.97932 to 1.97808, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 1.9781\n",
      "Epoch 85/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9753\n",
      "Epoch 00085: loss improved from 1.97808 to 1.97527, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 1.9753\n",
      "Epoch 86/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9708\n",
      "Epoch 00086: loss improved from 1.97527 to 1.97077, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 1.9708\n",
      "Epoch 87/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9675\n",
      "Epoch 00087: loss improved from 1.97077 to 1.96754, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 255ms/step - loss: 1.9675\n",
      "Epoch 88/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9657\n",
      "Epoch 00088: loss improved from 1.96754 to 1.96566, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 1.9657\n",
      "Epoch 89/100\n",
      "275/473 [================>.............] - ETA: 50s - loss: 1.9586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/473 [==============================] - ETA: 0s - loss: 1.9367\n",
      "Epoch 00099: loss improved from 1.93762 to 1.93668, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 1.9367\n",
      "Epoch 100/100\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.9317\n",
      "Epoch 00100: loss improved from 1.93668 to 1.93173, saving model to model_weights_saved.hdf5\n",
      "473/473 [==============================] - 121s 256ms/step - loss: 1.9317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2108207358>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model and let it train\n",
    "model.fit(X, y, epochs=100, batch_size=512, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile the model with saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the model back to characters\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: \n",
      "\" esidenceleghorndivulgedspeedilydeliveredfrenchgovernmentconsequentlyhiredvesselconveyconstantinoplec \"\n"
     ]
    }
   ],
   "source": [
    "# Random seed to help generate\n",
    "start = np.random.randint(0, len(x_data)-1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed: \")\n",
    "print(\"\\\"\", \"\".join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estowedsensationseveralseadysearseasedarticulatedearshallalonesaidshallsentedearshallalonesaidshallsentedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesenderedearlysensesen"
     ]
    }
   ],
   "source": [
    "# Generate the text\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1,len(pattern), 1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose = 0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
